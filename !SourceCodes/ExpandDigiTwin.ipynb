{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6c7f07-3cfb-4659-8165-0ffea65fbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing all needed libraries\n",
    "try:\n",
    "    # Summit-related imports\n",
    "    import summit\n",
    "    from summit import Runner\n",
    "    from summit.benchmarks import SnarBenchmark\n",
    "    from summit.domain import *\n",
    "    from summit.utils.dataset import DataSet\n",
    "    from summit.strategies import SOBO, MultitoSingleObjective, LHS\n",
    "\n",
    "    # External libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # File and path handling\n",
    "    import pathlib\n",
    "    import os\n",
    "    import logging\n",
    "    import re\n",
    "\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please install the required libraries before running the program.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6cc257-ec22-4631-b0de-2ad164c7e848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_domain(init_bounds_df):\n",
    "    \"\"\"\n",
    "    Create a Summit domain based on the provided boundaries DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - init_bounds_df (pd.DataFrame): DataFrame containing information about variable boundaries.\n",
    "\n",
    "    Returns:\n",
    "    - domain (Domain): The created Summit domain.\n",
    "    - obj_df (pd.DataFrame): DataFrame for objective variables.\n",
    "    - in_count (int): Count of input variables.\n",
    "    - out_count (int): Count of output variables.\n",
    "    \"\"\"\n",
    "    if not isinstance(init_bounds_df, pd.DataFrame) or init_bounds_df.empty:\n",
    "        raise ValueError(\"Invalid input: init_bounds_df must be a non-empty DataFrame.\")\n",
    "        \n",
    "    domain = Domain()\n",
    "    obj_df = pd.DataFrame()\n",
    "    obj_df = DataSet.from_df(obj_df)\n",
    "\n",
    "    in_count = 0\n",
    "    out_count = 0\n",
    "\n",
    "    for idx, row in init_bounds_df.iterrows():\n",
    "        name = row[0]\n",
    "        description = row[5]\n",
    "        data_type = row[1]\n",
    "\n",
    "        if data_type == 'Categorical':\n",
    "            levels = row[2].split(',')\n",
    "\n",
    "            domain += CategoricalVariable(\n",
    "                name = name,\n",
    "                description = description,\n",
    "                levels = levels\n",
    "            )\n",
    "            in_count += 1\n",
    "\n",
    "        elif data_type == 'Continuous':\n",
    "            bounds = [row[3], row[4]]\n",
    "\n",
    "            domain += ContinuousVariable(\n",
    "                name = name,\n",
    "                description = description,\n",
    "                bounds = bounds\n",
    "            )\n",
    "            in_count += 1\n",
    "\n",
    "        elif data_type == 'Objective':\n",
    "            bounds = [row[3], row[4]]\n",
    "            maximize = row[6]\n",
    "\n",
    "            domain += ContinuousVariable(\n",
    "                name = name,\n",
    "                description = description,\n",
    "                bounds = bounds,\n",
    "                is_objective = True,\n",
    "                maximize = maximize\n",
    "            )\n",
    "            out_count += 1\n",
    "\n",
    "            obj_df[(name, \"DATA\")] = \"\"\n",
    "        \n",
    "    return domain, obj_df, in_count, out_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024c281f-7e24-4d1b-9562-d8fad0e71c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data_df, init_bounds_df, out_count):\n",
    "    \"\"\"\n",
    "    Preprocess the input data.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df (pd.DataFrame): Input data DataFrame.\n",
    "    - init_bounds_df (pd.DataFrame): DataFrame containing information about variable boundaries.\n",
    "    - out_count (int): Count of output variables.\n",
    "\n",
    "    Returns:\n",
    "    - sorted_data_df (pd.DataFrame): Sorted data DataFrame.\n",
    "    - in_bounds_thresh_df (pd.DataFrame): Thresholds for initial boundary data.\n",
    "    - INIT_BOUNDS_THRESH_FRAC (float): Fraction used for calculating bounds thresholds.\n",
    "    - ach_func_bounds (list): Achievement function bounds.\n",
    "    - ach_func_thresh (float): Threshold for the achievement function.\n",
    "    \"\"\"   \n",
    "    # Constants\n",
    "    DATA_COL_NAME = ('Achievement_Function','DATA')\n",
    "    BOUNDS_COL_NAME = 'Threshold'\n",
    "    INIT_BOUNDS_THRESH_FRAC = 0.10\n",
    "    ACH_FUNC_THRESH_FRAC = 0.05\n",
    "    \n",
    "    # Achievement function bounds\n",
    "    ach_func_bounds = [-1,1] #Change this to automatic later on    \n",
    "    \n",
    "    # Calculate the achievement function\n",
    "    data_df[DATA_COL_NAME] = -(data_df.iloc[:, -2]/1e4)  + (data_df.iloc[:, -1]/100)\n",
    "    \n",
    "    # Copy the data DataFrame and sorts by the achievement function\n",
    "    sorted_data_df = data_df.sort_values(\n",
    "        by = DATA_COL_NAME,\n",
    "        ascending = False\n",
    "    ).copy()\n",
    "\n",
    "    # Create a copy of the initial bounds DataFrame\n",
    "    bounds_thresh_df = init_bounds_df.copy()\n",
    "    \n",
    "    # Calculate the threshold for the initial boundaries\n",
    "    bounds_thresh_df[BOUNDS_COL_NAME] = (init_bounds_df.iloc[:, 4] - init_bounds_df.iloc[:, 3])*INIT_BOUNDS_THRESH_FRAC\n",
    "\n",
    "    # Create a copy of the initial boundaries + threshold DataFrame and removes the output boundaries + threshold\n",
    "    in_bounds_thresh_df = bounds_thresh_df.iloc[:-(out_count)].copy()\n",
    "\n",
    "    # Calculate the achievement function threshold\n",
    "    ach_func_thresh = (ach_func_bounds[1] - ach_func_bounds[0])*ACH_FUNC_THRESH_FRAC\n",
    "    \n",
    "    return sorted_data_df, in_bounds_thresh_df, INIT_BOUNDS_THRESH_FRAC, ach_func_bounds, ach_func_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8bdd1a-fbf7-48f9-90c1-67b47145dc07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##All check functions\n",
    "def check_af_converg(sorted_data_df):\n",
    "    \"\"\"\n",
    "    Check the convergence of the achievement function for the top 3 rows.\n",
    "\n",
    "    Parameters:\n",
    "    - sorted_data_df (pd.DataFrame): Sorted data DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - avg_diff_top_3 (float): Average absolute difference of the achievement function for the top 3 rows from their mean.\n",
    "    \"\"\"\n",
    "    if len(sorted_data_df) < 3:\n",
    "        raise ValueError(\"Insufficient data for convergence check. Need at least 3 rows.\")\n",
    "\n",
    "    top_3_af = sorted_data_df.iloc[:3, -1]\n",
    "    avg_top_3 = top_3_af.mean()\n",
    "    avg_diff_top_3 = np.abs(top_3_af - avg_top_3).mean()\n",
    "\n",
    "    return avg_diff_top_3\n",
    "    \n",
    "def check_bounds(sorted_data_df, in_bounds_thresh_df, out_count):\n",
    "    \"\"\"\n",
    "    Check if conditions are close to the upper or lower bounds.\n",
    "\n",
    "    Parameters:\n",
    "    - sorted_data_df (pd.DataFrame): Sorted data DataFrame.\n",
    "    - in_bounds_thresh_df (pd.DataFrame): Thresholds for initial boundary data.\n",
    "    - out_count (int): Count of output variables.\n",
    "\n",
    "    Returns:\n",
    "    - close_to_max_bounds (list): Conditions close to the upper bounds.\n",
    "    - close_to_min_bounds (list): Conditions close to the lower bounds.\n",
    "    \"\"\"\n",
    "    if sorted_data_df.empty or in_bounds_thresh_df.empty:\n",
    "        raise ValueError(\"Input DataFrames cannot be empty.\")\n",
    "    \n",
    "    close_to_max_bounds = []\n",
    "    close_to_min_bounds = []\n",
    "    check_exp = sorted_data_df.iloc[0, 0:-(out_count + 2)].to_numpy()\n",
    "    \n",
    "    for idx, row in in_bounds_thresh_df.iterrows():\n",
    "        col_name = row['Condition']\n",
    "        min_bounds = row['BoundaryMin']\n",
    "        max_bounds = row['BoundaryMax']\n",
    "        thresh = row['Threshold']\n",
    "        val = check_exp[idx]\n",
    "        \n",
    "        if np.isfinite(val):\n",
    "            if (val - min_bounds) < thresh:\n",
    "                close_to_min_bounds.append(col_name)\n",
    "            elif (max_bounds - val) < thresh:\n",
    "                close_to_max_bounds.append(col_name)\n",
    "                \n",
    "    return close_to_max_bounds, close_to_min_bounds\n",
    "\n",
    "def check_max_possibility(sorted_data_df, ach_func_thresh, ach_func_bounds):\n",
    "    \"\"\"\n",
    "    Check if the achievement function is close to the upper bound.\n",
    "\n",
    "    Parameters:\n",
    "    - sorted_data_df (pd.DataFrame): Sorted data DataFrame.\n",
    "    - ach_func_thresh (float): Threshold for the achievement function.\n",
    "    - ach_func_bounds (list): Achievement function bounds.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if near the upper bound, False otherwise.\n",
    "    \"\"\"\n",
    "    if sorted_data_df.empty:\n",
    "        raise ValueError(\"Input DataFrame 'sorted_data_df' cannot be empty.\")\n",
    "    \n",
    "    if not ach_func_bounds or len(ach_func_bounds) != 2:\n",
    "        raise ValueError(\"Invalid 'ach_func_bounds'. It should be a list of two values representing bounds.\")\n",
    "    \n",
    "    if ach_func_thresh is None or not isinstance(ach_func_thresh, (int, float)):\n",
    "        raise ValueError(\"Invalid 'ach_func_thresh'. It should be a numeric value.\")\n",
    "    \n",
    "    upper_bound = ach_func_bounds[1]\n",
    "    achievement_function_value = sorted_data_df.iloc[0, -1]\n",
    "\n",
    "    if abs(achievement_function_value - upper_bound) < ach_func_thresh:\n",
    "        print('Near the maximum possibility. Optimization has ended.')\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def check_goal(sorted_data_df, GOAL):\n",
    "    \"\"\"\n",
    "    Check if the optimized result exceeds the specified goal.\n",
    "\n",
    "    Parameters:\n",
    "    - sorted_data_df (pd.DataFrame): Sorted data DataFrame.\n",
    "    - GOAL (float): Target goal.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the goal is achieved, False otherwise.\n",
    "    \"\"\"\n",
    "    if sorted_data_df.empty:\n",
    "        raise ValueError(\"Input DataFrame 'sorted_data_df' cannot be empty.\")\n",
    "\n",
    "    if GOAL is None or not isinstance(GOAL, (int, float)):\n",
    "        raise ValueError(\"Invalid 'GOAL'. It should be a numeric value.\")\n",
    "\n",
    "    # Check if the goal is achieved\n",
    "    if sorted_data_df.iloc[0, -1] < GOAL:\n",
    "        print('Optimized to the goal within given boundaries')\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def expand_bounds(sorted_data_df, in_bounds_thresh_df, init_bounds_df, EXPANSION_COEF, INIT_BOUNDS_THRESH_FRAC, out_count):\n",
    "    \"\"\"\n",
    "    Expand boundaries based on parameters near the bounds.\n",
    "\n",
    "    Parameters:\n",
    "    - sorted_data_df (pd.DataFrame): Sorted data DataFrame.\n",
    "    - in_bounds_thresh_df (pd.DataFrame): Thresholds for initial boundary data.\n",
    "    - init_bounds_df (pd.DataFrame): Initial boundaries DataFrame.\n",
    "    - EXPANSION_COEF (float): Expansion coefficient.\n",
    "    - INIT_BOUNDS_THRESH_FRAC (float): Fraction used for calculating bounds thresholds.\n",
    "    - out_count (int): Count of output variables.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if no parameters near the boundaries, False otherwise.\n",
    "    \"\"\"\n",
    "    if sorted_data_df.empty or in_bounds_thresh_df.empty or init_bounds_df.empty:\n",
    "        raise ValueError(\"Input DataFrames cannot be empty.\")\n",
    "        \n",
    "    # Check if there are parameters near the bounds\n",
    "    close_to_max_bounds, close_to_min_bounds = check_bounds(sorted_data_df, in_bounds_thresh_df, out_count)\n",
    "    \n",
    "    if not (close_to_max_bounds or close_to_min_bounds):\n",
    "        print('No parameters near the boundaries')\n",
    "        return True\n",
    "\n",
    "   # Expand the boundaries based on proximity to the bounds\n",
    "    for parameter in close_to_max_bounds:\n",
    "        param_bounds_row = init_bounds_df.loc[init_bounds_df.iloc[:, 0] == parameter].copy()\n",
    "        param_bounds_row.iloc[:, 4] += EXPANSION_COEF * (param_bounds_row.iloc[:, 4] - param_bounds_row.iloc[:, 3])\n",
    "        init_bounds_df.loc[init_bounds_df.iloc[:, 0] == parameter] = param_bounds_row\n",
    "\n",
    "    for parameter in close_to_min_bounds:\n",
    "        param_bounds_row = init_bounds_df.loc[init_bounds_df.iloc[:, 0] == parameter].copy()\n",
    "        param_bounds_row.iloc[:, 3] -= EXPANSION_COEF * (param_bounds_row.iloc[:, 4] - param_bounds_row.iloc[:, 3])\n",
    "        init_bounds_df.loc[init_bounds_df.iloc[:, 0] == parameter] = param_bounds_row\n",
    "\n",
    "    return False\n",
    "\n",
    "def find_last_iteration_with_log(log_file_path, log_message, EXPLORE_QUAN):\n",
    "    \"\"\"\n",
    "    Find the last iteration number with a specific log message in the log file.\n",
    "\n",
    "    Parameters:\n",
    "    - log_file_path (str): Path to the log file.\n",
    "    - log_message (str): Log message to search for.\n",
    "\n",
    "    Returns:\n",
    "    - int or None: Last iteration number with the log message, or None if not found.\n",
    "    \"\"\"\n",
    "    # Updated regex to capture iteration number with decimals\n",
    "    pattern = re.compile(r\"Iteration ([\\d.]+): Program started(.*?)Iteration \\1: Program completed\", re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        with open(log_file_path, 'r') as log_file:\n",
    "            log_content = log_file.read()\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: Log file '{log_file_path}' not found. Please check the file path.\")\n",
    "        return False, None\n",
    "\n",
    "    matches = re.findall(pattern, log_content)\n",
    "\n",
    "    # Reverse the order to start from the last iteration\n",
    "    matches.reverse()\n",
    "\n",
    "    for iteration, logs in matches[:EXPLORE_QUAN]:  # Look in the last EXPLORE_QUAN iterations\n",
    "        if log_message.lower() in logs.lower():  # Case-insensitive comparison\n",
    "            return True, int(float(iteration))\n",
    "\n",
    "    return False, None\n",
    "\n",
    "def check_steps(init_bounds_df, GOAL, EXPANSION_COEF, data_df, out_count, EXPLORE_QUAN, verbose = True):\n",
    "    \"\"\"\n",
    "    Check the optimization stage based on the current state of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - init_bounds_df (pd.DataFrame): Initial boundaries DataFrame.\n",
    "    - GOAL (float): Target goal.\n",
    "    - EXPANSION_COEF (float): Expansion coefficient.\n",
    "    - data_df (pd.DataFrame): Input data DataFrame.\n",
    "    - out_count (int): Count of output variables.\n",
    "    - EXPLORE_QUAN (int): Amount of iterations to explore\n",
    "    - verbose (bool): Whether to print detailed information.\n",
    "\n",
    "    Returns:\n",
    "    - int: Check step (1, 2, 3, 4, or 5).\n",
    "    \"\"\"\n",
    "\n",
    "    log_message = \"Expansion of bounds dataframe\"\n",
    "    sorted_data_df, in_bounds_thresh_df, INIT_BOUNDS_THRESH_FRAC, ach_func_bounds, ach_func_thresh = preprocess_data(data_df, init_bounds_df, out_count)\n",
    "\n",
    "    if check_max_possibility(sorted_data_df, ach_func_thresh, ach_func_bounds):\n",
    "        if verbose:\n",
    "            print('Near the maximum possibility. Optimization has ended.')\n",
    "        return CHECK_STEP_OPTIMIZATION_ENDS, None\n",
    "\n",
    "    log_found, last_iteration = find_last_iteration_with_log(log_file_path, log_message, EXPLORE_QUAN)\n",
    "\n",
    "    if log_found:\n",
    "        if verbose:\n",
    "            print(f\"Log message '{log_message}' found in iteration {last_iteration}.\")\n",
    "        return CHECK_STEP_LOG_MESSAGE_FOUND, last_iteration\n",
    "\n",
    "    elif check_af_converg(sorted_data_df) < (ach_func_thresh / 2):\n",
    "        if check_goal(sorted_data_df, GOAL):\n",
    "            if verbose:\n",
    "                print('Optimized to the goal within given boundaries.')\n",
    "            return CHECK_STEP_OPTIMIZATION_ENDS, None\n",
    "\n",
    "        elif not expand_bounds(sorted_data_df, in_bounds_thresh_df, init_bounds_df, EXPANSION_COEF, INIT_BOUNDS_THRESH_FRAC, out_count):\n",
    "            if verbose:\n",
    "                print('The boundaries have been expanded. Please continue optimization.')\n",
    "            return CHECK_STEP_EXPAND_BOUNDS, None\n",
    "\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('Optimized under the set goal. The reaction conditions have hit a plateau. Reevaluate the data.')\n",
    "            return CHECK_STEP_HIT_PLATEAU, None\n",
    "\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('Continue with the optimization process.')\n",
    "        return CHECK_STEP_CONTINUE_OPTIMIZATION, None\n",
    "\n",
    "# Constants for check steps\n",
    "CHECK_STEP_OPTIMIZATION_ENDS = 1\n",
    "CHECK_STEP_CONTINUE_OPTIMIZATION = 2\n",
    "CHECK_STEP_EXPAND_BOUNDS = 3\n",
    "CHECK_STEP_HIT_PLATEAU = 4\n",
    "CHECK_STEP_LOG_MESSAGE_FOUND = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afafbb05-981e-4daa-adfa-3e7c46438d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def it_count(start_exp_num, fin_exp_num, suggest_amount, project_name):\n",
    "    \"\"\"\n",
    "    Calculate iteration-related values based on the start and finish experiment numbers.\n",
    "\n",
    "    Parameters:\n",
    "    - start_exp_num (int): Starting experiment number.\n",
    "    - fin_exp_num (int): Finish experiment number.\n",
    "    - suggest_amount (int): Number of experiments to suggest.\n",
    "    - PROJECT_NAME (str): Name of the project.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tuple containing iteration number, model names, and iteration names.\n",
    "    \"\"\"\n",
    "\n",
    "    if fin_exp_num < start_exp_num:\n",
    "        raise ValueError(\"Finish experiment number should be greater than or equal to the start experiment number.\")\n",
    "\n",
    "    current_it = (fin_exp_num - start_exp_num) / suggest_amount\n",
    "    next_it = current_it + 1\n",
    "    prev_it = current_it - 1\n",
    "    \n",
    "    model_name = f\"{project_name}_Model_It{current_it}.json\"\n",
    "    prev_model_name = f\"{project_name}_Model_It{prev_it}.json\"\n",
    "    it_name = f\"{project_name}_Exp_It{next_it}.csv\"\n",
    "    prev_it_name = f\"{project_name}_Exp_It{current_it}.csv\"\n",
    "\n",
    "    return current_it, model_name, prev_model_name, it_name, prev_it_name\n",
    "\n",
    "\n",
    "def create_initial_dataset(domain, DATA_NAME, START_EXP_NUM, obj_df, data_path, data_name, random_seed = 808): \n",
    "    \"\"\"\n",
    "    Create the initial dataset file and DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - domain (dict): Domain for Latin Hypercube Sampling.\n",
    "    - DATA_NAME (str): Name of the dataset file.\n",
    "    - START_EXP_NUM (int): Starting experiment number.\n",
    "    - RANDOM_SEED (int): Seed for random number generation.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Initial experiment DataFrame.\n",
    "    \"\"\"\n",
    "    categorical_method = \"one-hot\"\n",
    "    \n",
    "    initial_strategy = LHS(\n",
    "        domain = domain, \n",
    "        random_state = np.random.RandomState(random_seed)\n",
    "    ) \n",
    "    \n",
    "    temp_start_exp_df = pd.DataFrame(initial_strategy.suggest_experiments(START_EXP_NUM)) \n",
    "    start_exp = pd.concat(\n",
    "        [temp_start_exp_df, obj_df],\n",
    "        axis = 1\n",
    "    ) \n",
    "    \n",
    "    # Check if the file already exists before overwriting\n",
    "    file_path = data_path / data_name\n",
    "    if file_path.exists():\n",
    "        overwrite = input(f\"The file '{data_name}' already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(\"Operation canceled.\")\n",
    "            return start_exp\n",
    "        \n",
    "    start_exp.to_csv(\n",
    "        data_path / DATA_NAME,\n",
    "        index = True\n",
    "    ) \n",
    "        \n",
    "    print(f\"The file '{DATA_NAME}' does not exist in the directory. A new file has been created, please update it with results\") \n",
    "    return start_exp\n",
    "\n",
    "#Function to do Summit optimization\n",
    "def perform_summit_optimization(data_df, suggest_amount, expression, domain, project_name, model_path, it_path, data_path, DATA_NAME, start_exp_num, fin_exp_num, current_it, model_name, prev_model_name, it_name, prev_it_name):\n",
    "    \"\"\"\n",
    "    Perform Summit optimization.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df (pd.DataFrame): Data DataFrame.\n",
    "    - suggest_amount (int): Number of experiments to suggest.\n",
    "    - expression (str): Mathematical expression for optimization.\n",
    "    - domain (dict): Domain for optimization.\n",
    "    - project_name (str): Name of the project.\n",
    "    - model_path (Path): Path to save/load optimization model.\n",
    "    - it_path (Path): Path to save iteration data.\n",
    "    - start_exp_num (int): Starting experiment number.\n",
    "    - fin_exp_num (int): Finish experiment number.\n",
    "    \"\"\"\n",
    "\n",
    "    if current_it == 0:\n",
    "\n",
    "        transform = MultitoSingleObjective(\n",
    "            domain = domain,\n",
    "            expression = expression,\n",
    "            maximize = False\n",
    "        )\n",
    "        \n",
    "        strategy = SOBO(\n",
    "            domain = domain,\n",
    "            transform = transform\n",
    "        )\n",
    "\n",
    "        new_it = strategy.suggest_experiments(\n",
    "            num_experiments = suggest_amount,\n",
    "            prev_res = data_df\n",
    "        )\n",
    "        \n",
    "        it_exp_df = pd.concat([data_df, new_it], axis = 0)\n",
    "        it_exp_df.reset_index(drop = True, inplace = True)\n",
    "        it_exp_df = it_exp_df.iloc[:, :-1]\n",
    "\n",
    "        data_path = os.path.join(data_path, DATA_NAME)\n",
    "        it_exp_df.to_csv(data_path, index=True)\n",
    "        \n",
    "        it_data_path = os.path.join(it_path, it_name)\n",
    "        new_it.to_csv(it_data_path)\n",
    "        \n",
    "        model_data_path = os.path.join(model_path, model_name)\n",
    "        strategy.save(model_data_path)\n",
    "        \n",
    "        print(\"Your first new experimental condition has been added to run. The model has been saved in the directory.\")\n",
    "        print(new_it)\n",
    "\n",
    "    else: \n",
    "\n",
    "        prev_it = data_df.iloc[-1:].copy()\n",
    "        prev_it.to_csv(it_path / prev_it_name)\n",
    "        strategy = SOBO.load(model_path / prev_model_name)\n",
    "        \n",
    "        new_it = strategy.suggest_experiments(\n",
    "            num_experiments = suggest_amount,\n",
    "            prev_res = prev_it\n",
    "        )\n",
    "        \n",
    "        it_exp_df = pd.concat([data_df, new_it], axis = 0)\n",
    "        it_exp_df.reset_index(drop = True, inplace = True)\n",
    "        it_exp_df = it_exp_df.iloc[:, :-1]\n",
    "\n",
    "        data_path = os.path.join(data_path, DATA_NAME)\n",
    "        it_exp_df.to_csv(data_path, index=True)\n",
    "        \n",
    "        it_data_path = os.path.join(it_path, it_name)\n",
    "        new_it.to_csv(it_data_path)\n",
    "        \n",
    "        model_data_path = os.path.join(model_path, model_name)\n",
    "        strategy.save(model_data_path)\n",
    "        \n",
    "        print(\"A new experimental condition has been added to run. The model has been saved in the directory.\")\n",
    "        print(new_it)\n",
    "\n",
    "def create_progress_graph(data_df, start_exp_num):\n",
    "    \"\"\"\n",
    "    Create a progress graph to visualize the achievement function over experiment numbers.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df (pd.DataFrame): Data DataFrame.\n",
    "    - start_exp_num (int): Starting experiment number.\n",
    "    \"\"\"\n",
    "    data_exp_num = data_df.index + 1\n",
    "    data_ach_func = pd.to_numeric(data_df.iloc[:, -1], errors='coerce')  # Convert to numeric, handle errors as NaN\n",
    "\n",
    "    # Split the data into two parts for better visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(\n",
    "        data_exp_num[:start_exp_num],\n",
    "        data_ach_func[:start_exp_num],\n",
    "        marker='o',\n",
    "        linestyle='-',\n",
    "        color='grey',\n",
    "        markersize=5,\n",
    "        linewidth=1,\n",
    "        label='Best Achievement (Initial)'\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        data_exp_num[start_exp_num:],\n",
    "        data_ach_func[start_exp_num:],\n",
    "        marker='o',\n",
    "        linestyle='-',\n",
    "        color='navy',\n",
    "        markersize=5,\n",
    "        linewidth=1,\n",
    "        label='Best Achievement (After Optimization)'\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Experiment Number')\n",
    "    plt.ylabel('Achievement Function')\n",
    "    plt.title('Experiment Results')\n",
    "    plt.legend()  # Add legend to differentiate initial and optimized achievements\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3549d0-0cf7-4a53-b003-af618d14e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Constants\n",
    "    file_path = os.path.join(data_path, DATA_NAME)\n",
    "    START_EXP_NUM = 10\n",
    "    SUGGEST_AMOUNT = 1\n",
    "    EXPRESSION = \"-sty/1e4+e_factor/100\"\n",
    "    GOAL = -1.1\n",
    "    EXPANSION_COEF = 1.5\n",
    "    EXPLORE_QUAN = 2\n",
    "    check_distance = None\n",
    "    \n",
    "    domain, obj_df, in_count, out_count = create_domain(init_bounds_df)\n",
    "    \n",
    "    # Determines if there is an initial dataset that exists\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.info(f\"Creating initial dataset: {DATA_NAME}\")\n",
    "        create_initial_dataset(domain, DATA_NAME, START_EXP_NUM, obj_df, data_path, DATA_NAME,)\n",
    "        return\n",
    "    \n",
    "    # Reads the initial dataset\n",
    "    data_df = DataSet.read_csv(data_path / DATA_NAME) \n",
    "    logging.info(f\"Reading dataset: {DATA_NAME}\")\n",
    "    \n",
    "    # Determines that amount of total experiments equal amount of completed experiments\n",
    "    exp_num = data_df.iloc[:, 0].count()\n",
    "    fin_exp_num = data_df.iloc[:, -1].count()\n",
    "    if exp_num - fin_exp_num != 0:\n",
    "        logging.warning(\"Complete experiments and update .csv file.\")\n",
    "        return\n",
    "    \n",
    "    # Calculates the current iteration as well as all the names of models and iterations needed\n",
    "    current_it, model_name, prev_model_name, it_name, prev_it_name = it_count(START_EXP_NUM, fin_exp_num, SUGGEST_AMOUNT, PROJECT_NAME)\n",
    "    logging.info(f\"Iteration {current_it}: Program started\")\n",
    "    \n",
    "    # Check the status of the optimization progress\n",
    "    check_step, check_value = check_steps(init_bounds_df, GOAL, EXPANSION_COEF, data_df, out_count, EXPLORE_QUAN)\n",
    "    logging.info(f\"This is check step: {check_step}\")\n",
    "    # Creates the progress graph\n",
    "    create_progress_graph(data_df, START_EXP_NUM)\n",
    "\n",
    "    # Reports what the status of the optimization progress is and halts the program or continues\n",
    "    if check_step == 1 or check_step == 4:\n",
    "        logging.warning(\"Optimization has been halted\")\n",
    "        return\n",
    "\n",
    "    if check_step == 3:\n",
    "        logging.info(\"Expansion of bounds dataframe\")\n",
    "        init_bounds_df.to_csv(data_path / BOUNDS_NAME, index=False)\n",
    "        domain, obj_df, in_count, out_count = create_domain(init_bounds_df)\n",
    "        check_distance = 0\n",
    "        \n",
    "    if check_step == 5:\n",
    "        logging.info(\"Exploration of expanded bounds\")\n",
    "        check_distance = current_it - check_value\n",
    "        print(check_distance)\n",
    "        \n",
    "    # Performs the primiary optimization \n",
    "    perform_summit_optimization(data_df, SUGGEST_AMOUNT, EXPRESSION, domain, PROJECT_NAME, model_path, it_path, START_EXP_NUM, fin_exp_num, current_it, model_name, prev_model_name, it_name, prev_it_name)\n",
    "    \n",
    "    # Log the iteration number at the end of the program\n",
    "    logging.info(f\"Iteration {current_it}: Program completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae22ca01-753a-4d1b-808f-1ab78eea3239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue with the optimization process.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/Summit3918/lib/python3.9/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/Summit3918/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/Summit3918/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:1577\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:1587\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 10.0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: File \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBOUNDS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is empty or in an invalid format.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is check step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Creates the progress graph\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mcreate_progress_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTART_EXP_NUM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Reports what the status of the optimization progress is and halts the program or continues\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m check_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "Cell \u001b[0;32mIn[5], line 165\u001b[0m, in \u001b[0;36mcreate_progress_graph\u001b[0;34m(data_df, start_exp_num)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Split the data into two parts for better visualization\u001b[39;00m\n\u001b[1;32m    162\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m    163\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\n\u001b[1;32m    164\u001b[0m     data_exp_num[:start_exp_num],\n\u001b[0;32m--> 165\u001b[0m     \u001b[43mdata_ach_func\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart_exp_num\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    166\u001b[0m     marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    167\u001b[0m     linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m     color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrey\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    169\u001b[0m     markersize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    170\u001b[0m     linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    171\u001b[0m     label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Achievement (Initial)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    174\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\n\u001b[1;32m    175\u001b[0m     data_exp_num[start_exp_num:],\n\u001b[1;32m    176\u001b[0m     data_ach_func[start_exp_num:],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m     label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Achievement (After Optimization)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    183\u001b[0m )\n\u001b[1;32m    185\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExperiment Number\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Summit3918/lib/python3.9/site-packages/pandas/core/series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values(key)\n\u001b[0;32m-> 1007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Summit3918/lib/python3.9/site-packages/pandas/core/series.py:1014\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_with\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# other: fancy integer or otherwise\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;66;03m# _convert_slice_indexer to determine if this slice is positional\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;66;03m#  or label based, and if the latter, convert to positional\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         slobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_slice_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgetitem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice(slobj)\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, ABCDataFrame):\n",
      "File \u001b[0;32m~/anaconda3/envs/Summit3918/lib/python3.9/site-packages/pandas/core/indexes/numeric.py:232\u001b[0m, in \u001b[0;36mNumericIndex._convert_slice_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m kind \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetitem\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# TODO: can we write this as a condition based on\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#  e.g. _should_fallback_to_positional?\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# We always treat __getitem__ slicing as label-based\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# translate to locations\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_convert_slice_indexer(key, kind\u001b[38;5;241m=\u001b[39mkind)\n",
      "File \u001b[0;32m~/anaconda3/envs/Summit3918/lib/python3.9/site-packages/pandas/core/indexes/base.py:6559\u001b[0m, in \u001b[0;36mIndex.slice_indexer\u001b[0;34m(self, start, end, step, kind)\u001b[0m\n\u001b[1;32m   6516\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6517\u001b[0m \u001b[38;5;124;03mCompute the slice indexer for input labels and step.\u001b[39;00m\n\u001b[1;32m   6518\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6555\u001b[0m \u001b[38;5;124;03mslice(1, 3, None)\u001b[39;00m\n\u001b[1;32m   6556\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6557\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecated_arg(kind, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice_indexer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 6559\u001b[0m start_slice, end_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_locs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6561\u001b[0m \u001b[38;5;66;03m# return a slice\u001b[39;00m\n\u001b[1;32m   6562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(start_slice):\n",
      "File \u001b[0;32m~/anaconda3/envs/Summit3918/lib/python3.9/site-packages/pandas/core/indexes/base.py:6773\u001b[0m, in \u001b[0;36mIndex.slice_locs\u001b[0;34m(self, start, end, step, kind)\u001b[0m\n\u001b[1;32m   6771\u001b[0m end_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 6773\u001b[0m     end_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_slice_bound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end_slice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6775\u001b[0m     end_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Summit3918/lib/python3.9/site-packages/pandas/core/indexes/base.py:6686\u001b[0m, in \u001b[0;36mIndex.get_slice_bound\u001b[0;34m(self, label, side, kind)\u001b[0m\n\u001b[1;32m   6683\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_searchsorted_monotonic(label, side)\n\u001b[1;32m   6684\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   6685\u001b[0m         \u001b[38;5;66;03m# raise the original KeyError\u001b[39;00m\n\u001b[0;32m-> 6686\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m   6688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(slc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   6689\u001b[0m     \u001b[38;5;66;03m# get_loc may return a boolean array, which\u001b[39;00m\n\u001b[1;32m   6690\u001b[0m     \u001b[38;5;66;03m# is OK as long as they are representable by a slice.\u001b[39;00m\n\u001b[1;32m   6691\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m is_bool_dtype(slc\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/Summit3918/lib/python3.9/site-packages/pandas/core/indexes/base.py:6680\u001b[0m, in \u001b[0;36mIndex.get_slice_bound\u001b[0;34m(self, label, side, kind)\u001b[0m\n\u001b[1;32m   6678\u001b[0m \u001b[38;5;66;03m# we need to look up the label\u001b[39;00m\n\u001b[1;32m   6679\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6680\u001b[0m     slc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   6682\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Summit3918/lib/python3.9/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 10"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configurable parameters\n",
    "base_path = pathlib.Path(\"/home/chaubt/Bao_Research/ChemistryOptimization/DataSets/ExpandDigiTwin\")\n",
    "PROJECT_NAME = \"ExpandDigiTwin\"\n",
    "BOUNDS_NAME = f\"{PROJECT_NAME}_Bounds.csv\"\n",
    "DATA_NAME = f\"{PROJECT_NAME}_Data.csv\"\n",
    "LOG_NAME = f\"{PROJECT_NAME}_Log.csv\"\n",
    "#BOUNDS_NAME = \"Nakul_Midazolam_BoundariesV2.csv\"\n",
    "#DATA_NAME = \"StartExp.csv\"\n",
    "\n",
    "# Folder Name\n",
    "DATA_DIR = \"Data\"\n",
    "MODEL_DIR = \"Models\"\n",
    "IT_DIR = \"IterData\"\n",
    "LOG_DIR = \"Logs\"\n",
    "\n",
    "# Function to create directory if it doesn't exist\n",
    "def create_directory(base_path, directory):\n",
    "    \"\"\"Create a directory if it doesn't exist.\"\"\"\n",
    "    dir_path = base_path / directory\n",
    "    if not dir_path.is_dir():\n",
    "        dir_path.mkdir(parents=True)\n",
    "        \n",
    "# Create directories\n",
    "for directory in [DATA_DIR, MODEL_DIR, IT_DIR, LOG_DIR]:\n",
    "    create_directory(base_path, directory)\n",
    "\n",
    "# Set data paths\n",
    "data_path = base_path / DATA_DIR\n",
    "model_path = base_path / MODEL_DIR\n",
    "it_path = base_path / IT_DIR\n",
    "log_path = base_path / LOG_DIR\n",
    "\n",
    "# Configure logging\n",
    "log_file_path = log_path / LOG_NAME\n",
    "logging.basicConfig(\n",
    "    filename = log_file_path,\n",
    "    level = logging.INFO,\n",
    "    format = \"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Load initial boundaries data\n",
    "try:\n",
    "    init_bounds_df = pd.read_csv(data_path / BOUNDS_NAME)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{BOUNDS_NAME}' not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"Error: File '{BOUNDS_NAME}' is empty or in an invalid format.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23215b45-fd1f-4ea6-a0c3-7e329e8708cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
